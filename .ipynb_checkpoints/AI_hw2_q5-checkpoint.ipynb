{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run below cells sequentially \n",
    "#### edited: train() referenced from train_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "class simpleprob1():\n",
    "  # all actions into one single state, the keystate, give a high reward\n",
    "\n",
    "    def __init__(self,numh,numw, keystate):\n",
    "  \n",
    "        self.numh=numh\n",
    "        self.numw=numw\n",
    "\n",
    "        if (keystate[0]<0) or (keystate[0]>=self.numh):\n",
    "            print('illegal')\n",
    "            exit()\n",
    "        if (keystate[1]<0) or (keystate[1]>=self.numw):\n",
    "            print('illegal')\n",
    "            exit()\n",
    "\n",
    "        #state space: set of tuples (h,w) 0<=h<=numh, 0<=w<=numw\n",
    "        self.statespace=[ (h,w) for h in range(self.numh) for w in range(self.numw) ]\n",
    " \n",
    "        self.statespace2index=dict()\n",
    "        for i,s in enumerate(self.statespace):\n",
    "            self.statespace2index[s]=i\n",
    "\n",
    "\n",
    "\n",
    "        self.actions=['stay','left','down','right','up']\n",
    "        self.actdict2index=dict()\n",
    "        for i,a in enumerate(self.actions):\n",
    "            self.actdict2index[a]=i\n",
    "\n",
    "\n",
    "        self.highrewardstate=keystate\n",
    "        self.rewardtogothere=10.\n",
    "    \n",
    "        #only for RL\n",
    "        #self.state=[np.random.randint(0,self.numh),np.random.randint(0,self.numw)]\n",
    "        self.reset()\n",
    "\n",
    "    def transition_deterministic(self,oldstate_index,action):\n",
    "    #P(s'|s,a) is 1 for one specific s'\n",
    "\n",
    "        if action not in self.actions:\n",
    "            print('illegal')\n",
    "            exit()\n",
    "\n",
    "\n",
    "        oldstate=self.statespace[oldstate_index]\n",
    "    \n",
    "        # all deterministic\n",
    "\n",
    "        if self.actdict2index[action]==0:\n",
    "            newstate=list(oldstate)\n",
    "\n",
    "        elif self.actdict2index[action]==1:\n",
    "            newstate=list(oldstate)\n",
    "            newstate[1]=min(self.numw-1,newstate[1]+1)\n",
    "\n",
    "\n",
    "        elif self.actdict2index[action]==2:\n",
    "            newstate=list(oldstate)\n",
    "            newstate[0]=min(self.numh-1,newstate[0]+1)\n",
    "\n",
    "\n",
    "        elif self.actdict2index[action]==3:\n",
    "            newstate=list(oldstate)\n",
    "            newstate[1]=max(0,newstate[1]-1)\n",
    "\n",
    "\n",
    "        elif self.actdict2index[action]==4:\n",
    "            newstate=list(oldstate)\n",
    "            newstate[0]=max(0,newstate[0]-1)\n",
    "\n",
    "        #can return probs or set of new states and probabilities\n",
    "\n",
    "        done=False # can play forever\n",
    "        return self.statespace2index[tuple(newstate)]\n",
    "  \n",
    "\n",
    "    def reward(self,oldstate_index,action,newstate_index):\n",
    "        #P(R|s,a)\n",
    "        onlygoalcounts=True\n",
    "\n",
    "        if False==onlygoalcounts: #one gets  a reward when one jumps into the golden state or stays there\n",
    "            r=self.tmpreward1(oldstate_index, action, newstate_index)\n",
    "        else: #one gets only a reward when one stays in the golden state\n",
    "            r=self.tmpreward2(oldstate_index, action, newstate_index) \n",
    "\n",
    "        return r\n",
    "  \n",
    "    def tmpreward1(self,oldstate_index,action,newstate_index):\n",
    "\n",
    "        newstate=self.statespace[newstate_index]\n",
    "        if (newstate[0]==self.highrewardstate[0]) and (newstate[1]==self.highrewardstate[1]):\n",
    "            return self.rewardtogothere\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def tmpreward2(self,oldstate_index,action,newstate_index):\n",
    "\n",
    "        newstate=self.statespace[newstate_index]\n",
    "        if (newstate[0]==self.highrewardstate[0]) and (newstate[1]==self.highrewardstate[1]) and (action=='stay'):\n",
    "            return self.rewardtogothere\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "\n",
    "\n",
    "  ##################################\n",
    "  # for RL\n",
    "  #####################################\n",
    "    def reset(self):\n",
    "        #randomly set start point\n",
    "        self.state=np.random.randint(0,len(self.statespace))\n",
    "        return self.state\n",
    "\n",
    "    def getstate(self):\n",
    "        return self.state\n",
    "\n",
    "    def step(self,action):\n",
    "        #print(self.state,action)\n",
    "        done=False\n",
    "        tmpstateind=self.transition_deterministic(self.state,action)\n",
    "        reward=self.reward(self.state, action, tmpstateind)\n",
    "\n",
    "        self.state=tmpstateind\n",
    "\n",
    "        return self.state, reward, done\n",
    "\n",
    "def plotonlyvalstable2b(qvals, simpleprob_instance,  block):\n",
    "    # input is numpy of shape (5,h,w)  \n",
    "    #plotted into 3x3 + boundary  qvals[c,h,w] c=center,l,d,r,up\n",
    "    plt.ion()\n",
    "\n",
    "    mh=simpleprob_instance.numh\n",
    "    mw=simpleprob_instance.numw\n",
    "\n",
    "\n",
    "    plotvals=-np.ones((mh,mw))\n",
    "    for i in range(len(simpleprob_instance.statespace)):\n",
    "        h=simpleprob_instance.statespace[i][0]\n",
    "        w=simpleprob_instance.statespace[i][1]\n",
    "        for c in range( len(simpleprob_instance.actions)):\n",
    "            plotvals[h,w]=np.max(qvals[ i,:])\n",
    "\n",
    "    #print(qvals)\n",
    "    plotvals = np.ma.masked_where(plotvals<0,plotvals)\n",
    "\n",
    "    fig= plt.figure(1)\n",
    "    plt.clf()\n",
    "    #fig, (ax0) = plt.subplots(1, 1)\n",
    "    ax0=plt.axes()\n",
    "    ax0.imshow(plotvals, cmap=plt.get_cmap('summer'),interpolation='nearest')\n",
    "\n",
    "    #c = ax0.pcolor(plotvals, edgecolors='white', linewidths=1)\n",
    "    #ax0.patch.set(hatch='xx', edgecolor='black', color='red')\n",
    "\n",
    "    for h in range(mh):\n",
    "        for w in range(mw):\n",
    "            printstr= \"{:.2f}\".format(plotvals[h,w])   \n",
    "            ax0.text( w, h ,printstr,ha=\"center\", va=\"center\", color=\"k\")\n",
    "\n",
    "\n",
    "    plt.draw()\n",
    "    plt.pause(0.001)\n",
    "    if True==block:\n",
    "        #pass\n",
    "        input(\"Press [enter] to continue.\")\n",
    "\n",
    "\n",
    "def plotmoves(statesseq, simpleprob_instance,  block):\n",
    "    fig= plt.figure(5)\n",
    "    ax0=plt.axes()\n",
    "\n",
    "    plt.clf()\n",
    "    mh=simpleprob_instance.numh\n",
    "    mw=simpleprob_instance.numw\n",
    "\n",
    "    for s in statesseq:\n",
    "\n",
    "        #plt.clf()\n",
    "\n",
    "        h=simpleprob_instance.statespace[s][0]\n",
    "        w=simpleprob_instance.statespace[s][1]\n",
    "        plotvals=np.zeros((mh,mw))\n",
    "        plotvals[h,w]=1\n",
    "        print(h,w)\n",
    "        ax0.imshow(plotvals, cmap=plt.get_cmap('summer'),interpolation='nearest')\n",
    "\n",
    "        plt.draw()\n",
    "\n",
    "        plt.pause(0.05)  # pause a bit so that plots are updated\n",
    "        if is_ipython:\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "\n",
    "def plot_rewards2(episode_rewards,means100):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "\n",
    "    plt.title('training or testing...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('averaged reward')\n",
    "    plt.plot( np.asarray( episode_rewards ))\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(episode_rewards) >= 100:\n",
    "        #means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        #means = torch.cat((torch.zeros(99), means))\n",
    "        #plt.plot(means.numpy())\n",
    "        mn=np.mean(episode_rewards[-100:])\n",
    "    else:\n",
    "        mn=np.mean(episode_rewards)\n",
    "    means100.append(mn)\n",
    "    plt.plot(means100)\n",
    "        #print('100 mean:')\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "\n",
    "class agent_Qlearn_thattable( ):\n",
    "    def __init__(self, simpleprob_instance):\n",
    "\n",
    "        self.gamma=0.9\n",
    "        self.softupdate_alpha=0.2\n",
    "\n",
    "        self.delta=1e-3 # Q-convergence\n",
    "\n",
    "        self.numactions=len(simpleprob_instance.actions)\n",
    "        self.statespace= [i for i in range(len(simpleprob_instance.statespace))]\n",
    "        self.Q=np.zeros(( len(self.statespace) , self.numactions  ))\n",
    "\n",
    "        #how should the eps for exploration vs exploitation decay?\n",
    "        self.epsforgreediness_start=0.9\n",
    "        self.epsforgreediness_end=0.01\n",
    "        self.epsforgreediness_maxdecaytime=100\n",
    "\n",
    "\n",
    "\n",
    "    def currenteps(self,episode_index):\n",
    "    \n",
    "        if episode_index<0:\n",
    "            v=self.epsforgreediness_end\n",
    "        else:\n",
    "            v=self.epsforgreediness_end + (self.epsforgreediness_start-self.epsforgreediness_end)* max(0,self.epsforgreediness_maxdecaytime-episode_index)/ float(self.epsforgreediness_maxdecaytime)\n",
    "\n",
    "        return v\n",
    "\n",
    "    def actionfromQ(self,state_index,episode_index):\n",
    "        #episode_index for decay\n",
    "        eps=self.currenteps(episode_index)\n",
    "\n",
    "        r=np.random.uniform(low=0.0, high=1.0)\n",
    "        if r> eps:\n",
    "              action= np.argmax(self.Q[state_index,:])\n",
    "        else:\n",
    "            action= np.random.randint(low=0,high=self.numactions)\n",
    "        return action\n",
    "\n",
    "    def train(self, simpleprob_instance):\n",
    "\n",
    "        numepisodes=120 #250\n",
    "        maxstepsperepisode=100\n",
    "    \n",
    "\n",
    "        episode_rewards=[]\n",
    "        means100=[]\n",
    "\n",
    "        for ep in range(numepisodes):\n",
    "\n",
    "            state_index=simpleprob_instance.reset()\n",
    "\n",
    "            avgreward=0      \n",
    "        for playstep in range(maxstepsperepisode):\n",
    "            #\n",
    "            #\n",
    "            #\n",
    "            # YOUR IMPLEMENTATION HERE\n",
    "            # 1. get actionfromQ()\n",
    "            # 2. take a using step() and action to get new state and reward \n",
    "            # 3. get qmax from the new state\n",
    "            # 4. do a q-value update\n",
    "            # 5. update the state_index to the newstate index\n",
    "            # 6. calculate average reward as below     \n",
    "            avgreward = avgreward*playstep/(playstep+1.) + reward*1.0/(playstep+1.)\n",
    "\n",
    "\n",
    "        # outside of playing one episode\n",
    "        episode_rewards.append(avgreward)\n",
    "        plot_rewards2(episode_rewards,means100)\n",
    "\n",
    "        print('episode',ep,'averaged reward',avgreward)\n",
    "\n",
    "        if ep%10==0:\n",
    "            plotonlyvalstable2b(self.Q, simpleprob_instance,  block=False)\n",
    "            \n",
    "    def train(self, simpleprob_instance):\n",
    "\n",
    "        numepisodes = 250  # 250\n",
    "        maxstepsperepisode = 100\n",
    "\n",
    "        episode_rewards = []\n",
    "        means100 = []\n",
    "        learning_rate = 0.01\n",
    "        discount = 0.6\n",
    "        for ep in range(numepisodes):\n",
    "\n",
    "            state_index = simpleprob_instance.reset()\n",
    "            \n",
    "            avgreward = 0\n",
    "            for playstep in range(maxstepsperepisode):\n",
    "                # YOUR IMPLEMENTATION HERE\n",
    "                # 1. get actionfromQ()\n",
    "                action_index = self.actionfromQ(state_index, episode_index=ep)\n",
    "                # 2. take a using step() and action to get new state and reward\n",
    "                action = simpleprob_instance.actions[action_index]\n",
    "                newstate_index, reward, done = simpleprob_instance.step(action)\n",
    "\n",
    "                # 3. get qmax from the new state\n",
    "                max_future_Q = max(self.Q[newstate_index])\n",
    "                current_Q = self.Q[state_index, action_index]\n",
    "                new_Q = (1 - learning_rate) * current_Q + learning_rate * (reward + discount * max_future_Q)\n",
    "\n",
    "                # 4. do a q-value update\n",
    "                self.Q[state_index, action_index] = new_Q\n",
    "\n",
    "                # 5. update the state_index to the newstate index\n",
    "                state_index = newstate_index\n",
    "\n",
    "                # 6. calculate average reward as below\n",
    "                avgreward = avgreward * playstep / (playstep + 1.) + reward * 1.0 / (playstep + 1.)\n",
    "\n",
    "            # print(\"Average Reward per episode\", avgreward)\n",
    "            # outside of playing one episode\n",
    "            episode_rewards.append(avgreward)\n",
    "            plot_rewards2(episode_rewards, means100)\n",
    "\n",
    "            print('episode', ep, 'averaged reward', avgreward)\n",
    "\n",
    "            if ep % 10 == 0:\n",
    "                plotonlyvalstable2b(self.Q, simpleprob_instance, block=False)\n",
    "\n",
    "\n",
    "    def runagent(self, simpleprob_instance):\n",
    "        maxstepsperepisode=20\n",
    "\n",
    "        state_index=simpleprob_instance.reset()\n",
    "\n",
    "        episode_rewards=[]\n",
    "        means100=[]\n",
    "        statesseq=[state_index]\n",
    "\n",
    "        avgreward=0\n",
    "        for playstep in range(maxstepsperepisode):\n",
    "\n",
    "            action_index=self.actionfromQ(state_index, episode_index=-1)\n",
    "            action= simpleprob_instance.actions[action_index]\n",
    "            #print('act',action)\n",
    "            newstate_index,reward,done=simpleprob_instance.step(action)\n",
    "\n",
    "            state_index=newstate_index\n",
    "\n",
    "            avgreward = avgreward*playstep/(playstep+1.) + reward*1.0/(playstep+1.)\n",
    "\n",
    "            statesseq.append(state_index)\n",
    "\n",
    "        # outside of playing one episode\n",
    "        episode_rewards.append(avgreward)\n",
    "        plot_rewards2(episode_rewards,means100)\n",
    "        print('post training run averaged reward',avgreward)\n",
    "        plotmoves(statesseq, simpleprob_instance,  block=False)\n",
    "\n",
    "\n",
    "def trainsth():\n",
    "\n",
    "    plotbig=True\n",
    "    showeveryiteration=True\n",
    "\n",
    "    problem=simpleprob1(5,6,keystate=[1,4])\n",
    "\n",
    "    ag=agent_Qlearn_thattable(problem)\n",
    "    ag.train(problem)\n",
    "\n",
    "    for i in range(3):\n",
    "        print('FINISHED')\n",
    "\n",
    "    ag.runagent( problem)\n",
    "\n",
    "    for i in range(3):\n",
    "        print('FINISHED')\n",
    "        input(\"Press [enter] to continue.\")\n",
    "\n",
    "\n",
    "#if __name__=='__main__':\n",
    "#    trainsth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainsth()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
